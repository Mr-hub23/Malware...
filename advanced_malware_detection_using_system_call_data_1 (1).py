# -*- coding: utf-8 -*-
"""Advanced Malware Detection Using System Call Data 1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1kU0uJiRurPq69uPrs0D6Tfh8b_kl-fLq
"""

# Step 1: Import Libraries
import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.ensemble import RandomForestClassifier
from sklearn.svm import SVC
from sklearn.neural_network import MLPClassifier
from sklearn.metrics import classification_report, accuracy_score, confusion_matrix, ConfusionMatrixDisplay
import joblib

# Step 2: Load the Dataset
file_path = 'dataset.csv'  # Update with your file path
data = pd.read_csv(file_path)
print(data.head())

# Step 3: Extract and Display Dataset Information
# 3.1: Number of instances
num_instances = data.shape[0]

# 3.2: Number of features (excluding the target column)
num_features = data.shape[1] - 1  # Exclude the target column

# 3.3: Number of instances from each class
class_counts = data['classification'].value_counts()

print(f"Number of instances: {num_instances}")
print(f"Number of features: {num_features}")
print(f"Number of instances from each class:\n{class_counts}")

# 3.4: Visualize the Number of Instances from Each Class Using a Pie Chart
plt.figure(figsize=(8, 8))
class_counts.plot.pie(autopct='%1.1f%%', startangle=90, colors=['#ff9999','#66b3ff'])
plt.title('Class Distribution: Malware vs Benign')
plt.ylabel('')  # Hides the y-label
plt.show()

# Step 4: Check for Missing Values
print("\nMissing values in the dataset:\n", data.isnull().sum())

# Step 5: Prepare the Features and Target Variables
X = data.drop('classification', axis=1)  # Assuming 'classification' is the target column
y = data['classification']

# Step 6: Split the Data into Training and Testing Sets (Ensuring Balanced Classes)
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)

# Step 7: Feature Scaling for Numerical Columns
numerical_cols = X_train.select_dtypes(include=['float', 'int']).columns
scaler = StandardScaler()
X_train[numerical_cols] = scaler.fit_transform(X_train[numerical_cols])
X_test[numerical_cols] = scaler.transform(X_test[numerical_cols])

# Step 8: Exploratory Data Analysis (EDA)

# 8.1: Visualize the distribution of the target variable
sns.countplot(x=y)
plt.title('Distribution of Target Variable')
plt.xlabel('Classification')
plt.ylabel('Count')
plt.show()

# 8.2: Correlation Matrix of Numerical Features
plt.figure(figsize=(12, 8))
sns.heatmap(data[numerical_cols].corr(), annot=True, fmt='.2f', cmap='coolwarm')
plt.title('Correlation Matrix')
plt.show()

# Highlight only strong correlations (above 0.5 or below -0.5)
plt.figure(figsize=(12, 8))
strong_corr = data[numerical_cols].corr().applymap(lambda x: x if abs(x) > 0.5 else 0)
sns.heatmap(strong_corr, annot=True, fmt='.2f', cmap='coolwarm', cbar=True)
plt.title('Strong Correlations (|corr| > 0.5)')
plt.show()

# Heatmap with diverging colormap
plt.figure(figsize=(12, 8))
sns.heatmap(data[numerical_cols].corr(), annot=True, fmt='.2f', cmap='RdBu_r', center=0)
plt.title('Diverging Correlation Matrix')
plt.show()

# 8.3: Pairplot for the First Few Features
sns.pairplot(data.iloc[:, :5], hue='classification')  # Adjust column indices as needed
plt.show()

# 8.4: Histograms for the First Few Features
data.iloc[:, :5].hist(bins=30, figsize=(10, 8))
plt.suptitle('Feature Distribution')
plt.show()

# Step 9: Data Preprocessing
# Drop the 'hash' column as it is non-numeric and not useful for modeling
X_train = X_train.drop(columns=['hash'])
X_test = X_test.drop(columns=['hash'])

# Step 10: Train and Evaluate the Random Forest Model
rf_model = RandomForestClassifier(random_state=42)
rf_model.fit(X_train, y_train)
y_pred_rf = rf_model.predict(X_test)

# 10.1: Random Forest Performance Report
print("Random Forest Classifier Report:\n")
print(classification_report(y_test, y_pred_rf))
print(f"Accuracy: {accuracy_score(y_test, y_pred_rf)}")

# 10.2: Confusion Matrix for Random Forest
cm_rf = confusion_matrix(y_test, y_pred_rf)
disp_rf = ConfusionMatrixDisplay(confusion_matrix=cm_rf, display_labels=rf_model.classes_)
disp_rf.plot()
plt.title('Confusion Matrix: Random Forest')
plt.show()

# Step 11: Train and Evaluate the SVM Model
svm_model = SVC(random_state=42)
svm_model.fit(X_train, y_train)
y_pred_svm = svm_model.predict(X_test)

# 11.1: SVM Performance Report
print("SVM Classifier Report:\n")
print(classification_report(y_test, y_pred_svm))
print(f"Accuracy: {accuracy_score(y_test, y_pred_svm)}")

# 11.2: Confusion Matrix for SVM
cm_svm = confusion_matrix(y_test, y_pred_svm)
disp_svm = ConfusionMatrixDisplay(confusion_matrix=cm_svm, display_labels=svm_model.classes_)
disp_svm.plot()
plt.title('Confusion Matrix: SVM')
plt.show()

# Step 12: Train and Evaluate the Neural Network Model
nn_model = MLPClassifier(random_state=42, max_iter=300)
nn_model.fit(X_train, y_train)
y_pred_nn = nn_model.predict(X_test)

# 12.1: Neural Network Performance Report
print("Neural Network Classifier Report:\n")
print(classification_report(y_test, y_pred_nn))
print(f"Accuracy: {accuracy_score(y_test, y_pred_nn)}")

# 12.2: Confusion Matrix for Neural Network
cm_nn = confusion_matrix(y_test, y_pred_nn)
disp_nn = ConfusionMatrixDisplay(confusion_matrix=cm_nn, display_labels=nn_model.classes_)
disp_nn.plot()
plt.title('Confusion Matrix: Neural Network')
plt.show()

# Step 13: Compare and Discuss Results

# 13.1: Compare Model Performance
models = ['Random Forest', 'SVM', 'Neural Network']
accuracies = [accuracy_score(y_test, y_pred_rf), accuracy_score(y_test, y_pred_svm), accuracy_score(y_test, y_pred_nn)]
plt.figure(figsize=(10, 6))
plt.bar(models, accuracies, color=['blue', 'orange', 'green'])
plt.ylim(0, 1)
plt.title('Comparison of Model Accuracies')
plt.ylabel('Accuracy')
plt.show()

fig, ax = plt.subplots(1, 3, figsize=(18, 6))

# Random Forest Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_rf), annot=True, fmt="d", cmap="Blues", ax=ax[0])
ax[0].set_title('Random Forest')
ax[0].set_xlabel('Predicted Labels')
ax[0].set_ylabel('True Labels')

# SVM Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_svm), annot=True, fmt="d", cmap="Oranges", ax=ax[1])
ax[1].set_title('SVM')
ax[1].set_xlabel('Predicted Labels')
ax[1].set_ylabel('True Labels')

# Neural Network Confusion Matrix
sns.heatmap(confusion_matrix(y_test, y_pred_nn), annot=True, fmt="d", cmap="Greens", ax=ax[2])
ax[2].set_title('Neural Network')
ax[2].set_xlabel('Predicted Labels')
ax[2].set_ylabel('True Labels')

plt.tight_layout()
plt.show()

from sklearn.metrics import roc_curve, auc

# Assuming 'malware' is the positive class
y_test_binary = (y_test == 'malware').astype(int)

# Compute ROC curve and ROC area for each model
fpr_rf, tpr_rf, _ = roc_curve(y_test_binary, rf_model.predict_proba(X_test)[:, 1])
roc_auc_rf = auc(fpr_rf, tpr_rf)

fpr_svm, tpr_svm, _ = roc_curve(y_test_binary, svm_model.decision_function(X_test))
roc_auc_svm = auc(fpr_svm, tpr_svm)

fpr_nn, tpr_nn, _ = roc_curve(y_test_binary, nn_model.predict_proba(X_test)[:, 1])
roc_auc_nn = auc(fpr_nn, tpr_nn)


# Plot of a ROC curve for each model
plt.figure(figsize=(10, 8))
plt.plot(fpr_rf, tpr_rf, color='blue', lw=2, label='Random Forest (area = %0.2f)' % roc_auc_rf)
plt.plot(fpr_svm, tpr_svm, color='orange', lw=2, label='SVM (area = %0.2f)' % roc_auc_svm)
plt.plot(fpr_nn, tpr_nn, color='green', lw=2, label='Neural Network (area = %0.2f)' % roc_auc_nn)
plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver Operating Characteristic')
plt.legend(loc="lower right")
plt.show()

# Visualization: Precision, Recall, and F1-Score by Model
import numpy as np

# Collect metrics
models = ['Random Forest', 'SVM', 'Neural Network']
precision = [classification_report(y_test, y_pred_rf, output_dict=True)['weighted avg']['precision'],
             classification_report(y_test, y_pred_svm, output_dict=True)['weighted avg']['precision'],
             classification_report(y_test, y_pred_nn, output_dict=True)['weighted avg']['precision']]

recall = [classification_report(y_test, y_pred_rf, output_dict=True)['weighted avg']['recall'],
          classification_report(y_test, y_pred_svm, output_dict=True)['weighted avg']['recall'],
          classification_report(y_test, y_pred_nn, output_dict=True)['weighted avg']['recall']]

f1_score = [classification_report(y_test, y_pred_rf, output_dict=True)['weighted avg']['f1-score'],
            classification_report(y_test, y_pred_svm, output_dict=True)['weighted avg']['f1-score'],
            classification_report(y_test, y_pred_nn, output_dict=True)['weighted avg']['f1-score']]

# Visualization
x = np.arange(len(models))  # the label locations
width = 0.2  # the width of the bars

fig, ax = plt.subplots(figsize=(12, 6))
rects1 = ax.bar(x - width, precision, width, label='Precision', color='blue')
rects2 = ax.bar(x, recall, width, label='Recall', color='orange')
rects3 = ax.bar(x + width, f1_score, width, label='F1-score', color='green') # Use f1_score instead of f1

# Add some text for labels, title and custom x-axis tick labels, etc.
ax.set_ylabel('Scores')
ax.set_title('Precision, Recall, and F1-Score by Model')
ax.set_xticks(x)
ax.set_xticklabels(models)
ax.legend()

fig.tight_layout()
plt.ylim(0, 1)
plt.show()

# 13.2: Discuss Potential Improvements
# - Consider hyperparameter tuning to further improve each model's performance.
# - Feature engineering could be explored to create new features or modify existing ones for better model performance.
# - Address potential class imbalance if it exists by applying techniques such as oversampling, undersampling, or using class weights.

# Step 14: Save the Models (Optional)

# Save the Random Forest model
joblib.dump(rf_model, 'random_forest_model.pkl')

# Save the SVM model
joblib.dump(svm_model, 'svm_model.pkl')

# Save the Neural Network model
joblib.dump(nn_model, 'neural_network_model.pkl')

print("Models saved successfully.")